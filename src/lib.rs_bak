//! FlatMapOf: a high-performance flat hash map using per-bucket seqlock, ported from Go's FlatMapOf.
//! Simplified, Rust-idiomatic API focused on speed.

use std::hash::BuildHasherDefault;
use std::mem::MaybeUninit;
use std::sync::atomic::{AtomicBool, AtomicPtr, AtomicU64, Ordering};

use std::cell::UnsafeCell;
use std::hash::BuildHasher;

use ahash::AHasher;
const ENTRIES_PER_BUCKET: usize = 7; // op byte = highest, keep 7 data bytes like Go (opByteIdx=7)
const META_MASK: u64 = 0x0080_8080_8080_8080;
const EMPTY_SLOT: u8 = 0;
const SLOT_MASK: u8 = 0x80; // mark bit in meta byte
const LOAD_FACTOR: f64 = 0.75;
const SHRINK_FRACTION: usize = 8;
const MIN_TABLE_LEN: usize = 32;
const OP_LOCK_MASK: u64 = 0x8000_0000_0000_0000; // highest meta byte's 0x80 bit acts as root lock
const NUM_STRIPES: usize = 64;
const HASH_INIT_FLAG: u64 = 0x8000_0000_0000_0000; // highest bit indicates initialized entry

#[inline]
fn next_pow2(mut n: usize) -> usize {
    if n < 2 {
        return 2;
    }
    n -= 1;
    n |= n >> 1;
    n |= n >> 2;
    n |= n >> 4;
    n |= n >> 8;
    n |= n >> 16;
    if usize::BITS == 64 {
        n |= n >> 32;
    }
    n + 1
}

#[inline]
fn calc_table_len(size_hint: usize) -> usize {
    let min_cap = (size_hint as f64 * (1.0 / (ENTRIES_PER_BUCKET as f64 * LOAD_FACTOR))) as usize;
    let base = min_cap.max(MIN_TABLE_LEN);
    next_pow2(base)
}

pub struct FlatMap<K, V> {
    table: AtomicPtr<Table<K, V>>,
    old_tables: std::cell::UnsafeCell<Vec<Box<Table<K, V>>>>,
    resize_lock: AtomicBool,
    shrink_on: bool,
    hasher: BuildHasherDefault<AHasher>,
}

// SAFETY: FlatMap coordinates concurrent access via per-bucket seqlocks, a root bucket op lock, and a resize_lock guarding table swaps.
// The UnsafeCell around the Arc<Table> is only mutated under resize_lock and never moved; readers clone the Arc which is safe.
unsafe impl<K: Send, V: Send> Send for FlatMap<K, V> {}
unsafe impl<K: Sync, V: Sync> Sync for FlatMap<K, V> {}

struct Table<K, V> {
    buckets: Vec<Bucket<K, V>>,
    mask: usize,
    counters: [AtomicU64; NUM_STRIPES],
}

#[repr(align(8))]
struct Bucket<K, V> {
    seq: AtomicU64,
    meta: AtomicU64,
    next: AtomicPtr<Bucket<K, V>>, // was: UnsafeCell<Option<Box<Bucket<K, V>>>>
    entries: UnsafeCell<[Entry<K, V>; ENTRIES_PER_BUCKET]>,
}

// SAFETY: We provide per-bucket seqlock (seq) and atomic meta updates, and never move buckets after creation.
// Concurrent access is coordinated via lock()/unlock() and Acquire/Release fences.
unsafe impl<K: Send, V: Send> Send for Bucket<K, V> {}
unsafe impl<K: Sync, V: Sync> Sync for Bucket<K, V> {}

#[cfg_attr(feature = "padding", repr(align(64)))]
struct Entry<K, V> {
    hash: u64, // Highest bit indicates if entry is initialized, remaining 63 bits are actual hash
    key: MaybeUninit<K>,
    val: MaybeUninit<V>,
}

impl<K, V> Default for Entry<K, V> {
    fn default() -> Self {
        Self {
            hash: 0, // 0 indicates empty slot (no HASH_INIT_FLAG)
            key: MaybeUninit::uninit(),
            val: MaybeUninit::uninit(),
        }
    }
}

/// Operation type for Process and RangeProcess methods
#[derive(Copy, Clone, Debug, Eq, PartialEq)]
pub enum Op {
    /// Cancel the operation, leaving the entry unchanged
    Cancel,
    /// Update the entry with the new value
    Update,
    /// Delete the entry from the map
    Delete,
}

impl<K: Clone, V: Clone> Clone for Bucket<K, V> {
    fn clone(&self) -> Self {
        // shallow clone meta/seq, deep-copy entries, ignore next for table swap simplicity
        let mut entries_arr: [Entry<K, V>; ENTRIES_PER_BUCKET] =
            std::array::from_fn(|_| Entry::default());
        let src_entries = unsafe { &*self.entries.get() };
        let meta = self.meta.load(Ordering::Relaxed);
        let mut marked = meta & META_MASK;
        while marked != 0 {
            let j = first_marked_byte_index(marked);
            entries_arr[j].hash = src_entries[j].hash; // Copy the full hash including init flag
            unsafe {
                entries_arr[j]
                    .key
                    .as_mut_ptr()
                    .write((*src_entries[j].key.assume_init_ref()).clone());
                entries_arr[j]
                    .val
                    .as_mut_ptr()
                    .write((*src_entries[j].val.assume_init_ref()).clone());
            }
            marked &= marked - 1;
        }
        Bucket {
            seq: AtomicU64::new(0), // Reset seq for new bucket
            meta: AtomicU64::new(meta),
            next: AtomicPtr::new(std::ptr::null_mut()), // reset chain; will be rebuilt on resize path
            entries: UnsafeCell::new(entries_arr),
        }
    }
}

impl<K: Eq + std::hash::Hash + std::clone::Clone, V: std::clone::Clone> FlatMap<K, V> {
    pub fn new() -> Self {
        Self::with_capacity(0)
    }
    pub fn with_capacity(size_hint: usize) -> Self {
        let len = calc_table_len(size_hint);
        let mut buckets = Vec::with_capacity(len);
        for _ in 0..len {
            buckets.push(Bucket::new());
        }
        let table = Table {
            buckets,
            mask: len - 1,
            counters: std::array::from_fn(|_| AtomicU64::new(0)),
        };
        let table_ptr = Box::into_raw(Box::new(table));
        Self {
            table: AtomicPtr::new(table_ptr),
            old_tables: std::cell::UnsafeCell::new(Vec::new()),
            resize_lock: AtomicBool::new(false),
            shrink_on: false,
            hasher: BuildHasherDefault::<AHasher>::default(),
        }
    }
    pub fn enable_shrink(mut self, enable: bool) -> Self {
        self.shrink_on = enable;
        self
    }
    pub fn get(&self, key: &K) -> Option<V>
    where
        V: Clone,
    {
        let table_arc = self.seq_load_table();
        let table = &*table_arc;
        let (hash64, hash_u8) = self.hash_pair(key);
        let idx = self.h1_mask(hash64, table.mask);
        let root = &table.buckets[idx];
        let h2w = broadcast(hash_u8);

        // Try lockfree read first (like Go version)
        let mut b = root;
        loop {
            let mut spins = 0;
            'retry: loop {
                let s1 = b.seq.load(Ordering::Acquire);
                if (s1 & 1) != 0 {
                    // writer in progress
                    if try_spin(&mut spins) {
                        continue 'retry;
                    }
                    // Too many spins, fallback to locked read
                    break 'retry;
                }

                let meta = b.meta.load(Ordering::Acquire);
                let mut marked = mark_zero_bytes(meta ^ h2w);

                // Copy entries while seqlock is stable (like Go version)
                while marked != 0 {
                    let j = first_marked_byte_index(marked);
                    let entries_ref = unsafe { &*b.entries.get() };
                    let e = &entries_ref[j];

                    // Copy entire entry first (like Go: e := *b.At(j))
                    let copied_entry = Entry {
                        hash: e.hash,
                        key: unsafe { std::ptr::read(&e.key) },
                        val: unsafe { std::ptr::read(&e.val) },
                    };

                    // Check seqlock immediately after copying (like Go version)
                    let s2 = b.seq.load(Ordering::Acquire);
                    if s1 != s2 {
                        if try_spin(&mut spins) {
                            continue 'retry;
                        }
                        // Too many spins, fallback to locked read
                        break 'retry;
                    }

                    // Now validate the copied entry
                    if copied_entry.is_occupied() && copied_entry.equal_hash(hash64) {
                        // SAFETY: entry is occupied and hash matches, so key should be initialized
                        unsafe {
                            let copied_key = copied_entry.key.assume_init_ref();
                            if *copied_key == *key {
                                // SAFETY: if key is valid, value should also be initialized
                                let copied_val = copied_entry.val.assume_init_ref().clone();
                                return Some(copied_val);
                            }
                        }
                    }

                    marked &= marked - 1;
                }

                // No match found in this bucket, move to next
                let next_ptr = b.next.load(Ordering::Acquire);
                if !next_ptr.is_null() {
                    b = unsafe { &*next_ptr };
                    continue 'retry;
                } else {
                    // End of chain, not found
                    return None;
                }
            }

            // Fallback: locked read under root bucket lock for consistency
            root.lock();
            let mut bb = root;
            loop {
                let meta_locked = bb.meta.load(Ordering::Relaxed);
                let mut marked_locked = mark_zero_bytes(meta_locked ^ h2w);
                let entries_ref_locked = unsafe { &*bb.entries.get() };
                while marked_locked != 0 {
                    let j = first_marked_byte_index(marked_locked);
                    let e = &entries_ref_locked[j];

                    // Check if this slot is actually occupied by checking meta byte and hash match
                    let slot_meta = (meta_locked >> (j * 8)) & 0xFF;
                    if slot_meta & (SLOT_MASK as u64) != 0
                        && e.is_occupied()
                        && e.equal_hash(hash64)
                    {
                        // SAFETY: Under lock, slot is marked as occupied, and hash matches
                        unsafe {
                            let key_ref = e.key.assume_init_ref();
                            if *key_ref == *key {
                                let result = e.val.assume_init_ref().clone();
                                root.unlock();
                                return Some(result);
                            }
                        }
                    }
                    marked_locked &= marked_locked - 1;
                }
                let next_ptr_locked = bb.next.load(Ordering::Acquire);
                if !next_ptr_locked.is_null() {
                    bb = unsafe { &*next_ptr_locked };
                } else {
                    break;
                }
            }
            root.unlock();
            return None;
        }
    }
    pub fn insert(&self, key: K, val: V) -> Option<V>
    where
        V: Clone,
    {
        self.process(key, |_| (Op::Update, Some(val.clone()))).0
    }
    pub fn remove(&self, key: K) -> Option<V>
    where
        V: Clone,
    {
        self.process(key, |_| (Op::Delete, None)).0
    }
    pub fn get_or_insert_with<F: FnOnce() -> V>(&self, key: K, f: F) -> (V, bool)
    where
        V: Clone,
    {
        // First try a simple get
        if let Some(existing) = self.get(&key) {
            return (existing, true);
        }

        // Use process method - f will only be called once now that process doesn't retry after insertion
        let mut f_option = Some(f);
        let (old_val, new_val) = self.process(key, |old| {
            if let Some(v) = old {
                (Op::Cancel, Some(v))
            } else {
                // Call f only once
                let computed = f_option.take().unwrap()();
                (Op::Update, Some(computed))
            }
        });

        if let Some(old) = old_val {
            (old, true)
        } else if let Some(new) = new_val {
            (new, false)
        } else {
            unreachable!("get_or_insert_with should always return a value")
        }
    }
    pub fn for_each<F: FnMut(&K, &V) -> bool>(&self, mut f: F) {
        // 在根桶锁下收集克隆，避免并发撕裂，然后在解锁后回调用户闭包
        let table_arc = self.seq_load_table();
        let table = &*table_arc;
        for i in 0..table.buckets.len() {
            let root = &table.buckets[i];
            root.lock();
            let mut b = root;
            let mut items: Vec<(K, V)> = Vec::new();
            loop {
                let entries_ref = unsafe { &*b.entries.get() };
                let meta = b.meta.load(Ordering::Relaxed);
                let mut marked = meta & META_MASK;
                while marked != 0 {
                    let j = first_marked_byte_index(marked);
                    let e = &entries_ref[j];
                    // Only access key/value if slot is marked as occupied in meta and entry is occupied
                    if (meta >> (j * 8)) & 0x80 != 0 && e.is_occupied() {
                        let k = unsafe { e.key.assume_init_ref().clone() };
                        let v = unsafe { e.val.assume_init_ref().clone() };
                        items.push((k, v));
                    }
                    marked &= marked - 1;
                }
                let next_ptr = b.next.load(Ordering::Acquire);
                if !next_ptr.is_null() {
                    b = unsafe { &*next_ptr };
                } else {
                    break;
                }
            }
            root.unlock();
            for (k, v) in items {
                if !f(&k, &v) {
                    return;
                }
            }
        }
    }
    // Rust风格迭代器（基于收集，简单安全）
    pub fn iter(&self) -> impl Iterator<Item = (K, V)>
    where
        K: Clone,
        V: Clone,
    {
        let mut items: Vec<(K, V)> = Vec::new();
        self.for_each(|k, v| {
            items.push((k.clone(), v.clone()));
            true
        });
        items.into_iter()
    }

    /// Process applies a compute-style update to a specific key.
    /// The function `f` receives the current value (if any) and returns an Op and new value.
    /// Returns (old_value, new_value) where old_value is the previous value if any.
    pub fn process<F>(&self, key: K, mut f: F) -> (Option<V>, Option<V>)
    where
        F: FnMut(Option<V>) -> (Op, Option<V>),
        V: Clone,
    {
        let (hash64, h2) = self.hash_pair(&key);

        loop {
            let table = self.seq_load_table();
            let idx = self.h1_mask(hash64, table.mask);
            let root = &table.buckets[idx];

            root.lock();

            // Check if table was swapped during lock acquisition
            let current_table = self.seq_load_table();
            if !std::ptr::eq(table, current_table) {
                root.unlock();
                continue; // Retry with new table
            }

            // Search for existing key and track empty slots
            let mut b = root;
            let mut found_info: Option<(*mut Bucket<K, V>, usize)> = None;
            let mut empty_slot_info: Option<(*const Bucket<K, V>, usize)> = None;
            let h2w = broadcast(h2);

            'search_loop: loop {
                let entries = unsafe { &*b.entries.get() };
                let meta = b.meta.load(Ordering::Acquire);
                let mut marked = mark_zero_bytes(meta ^ h2w);

                while marked != 0 {
                    let slot = first_marked_byte_index(marked);
                    if slot >= ENTRIES_PER_BUCKET {
                        break;
                    }

                    let entry = &entries[slot];
                    if entry.is_occupied() && entry.equal_hash(hash64) {
                        let entry_key = unsafe { entry.key.assume_init_ref() };
                        if entry_key == &key {
                            found_info = Some((b as *const _ as *mut _, slot));
                            break 'search_loop;
                        }
                    }
                    marked &= marked - 1;
                }

                // Track empty slots for potential insertion
                if empty_slot_info.is_none() {
                    let empty = (!meta) & META_MASK;
                    if empty != 0 {
                        empty_slot_info = Some((b as *const _, first_marked_byte_index(empty)));
                    }
                }

                let next_ptr = b.next.load(Ordering::Acquire);
                if !next_ptr.is_null() {
                    b = unsafe { &*next_ptr };
                } else {
                    break;
                }
            }

            if let Some((bucket_ptr, slot)) = found_info {
                // Key found - process existing entry
                let bucket = unsafe { &*bucket_ptr };
                let entries = unsafe { &mut *bucket.entries.get() };
                let entry = &mut entries[slot];
                let old_val = unsafe { entry.val.assume_init_ref().clone() };
                let (op, new_val) = f(Some(old_val.clone()));

                match op {
                    Op::Cancel => {
                        root.unlock();
                        return (Some(old_val.clone()), Some(old_val));
                    }
                    Op::Update => {
                        if let Some(new_v) = new_val {
                            // Use seqlock to protect the write operation (like Go)
                            let seq = bucket.seq.load(Ordering::Relaxed);
                            bucket.seq.store(seq + 1, Ordering::Release); // Start write (make odd)
                            entry.val = MaybeUninit::new(new_v.clone());
                            bucket.seq.store(seq + 2, Ordering::Release); // End write (make even)
                            root.unlock();
                            return (Some(old_val), Some(new_v));
                        } else {
                            root.unlock();
                            return (Some(old_val), None);
                        }
                    }
                    Op::Delete => {
                        // Clear the entry
                        entry.clear();
                        unsafe {
                            std::ptr::drop_in_place(entry.key.as_mut_ptr());
                            std::ptr::drop_in_place(entry.val.as_mut_ptr());
                        }

                        // Update meta to clear the slot - set the meta byte to EMPTY_SLOT (0)
                        let meta = bucket.meta.load(Ordering::Acquire);
                        let new_meta = set_byte(meta, EMPTY_SLOT, slot);
                        bucket.meta.store(new_meta, Ordering::Release);

                        // Update counter
                        let stripe = hash64 as usize % NUM_STRIPES;
                        table.counters[stripe].fetch_sub(1, Ordering::Relaxed);

                        root.unlock();
                        self.maybe_resize_after_remove(table);
                        return (Some(old_val), None);
                    }
                }
            } else {
                // Key not found - process new entry
                let (op, new_val) = f(None);
                match op {
                    Op::Cancel => {
                        root.unlock();
                        return (None, None);
                    }
                    Op::Update => {
                        if let Some(new_v) = new_val {
                            // Insert new entry directly under lock (like Go version)
                            if let Some((empty_bucket_ptr, empty_slot)) = empty_slot_info {
                                // Insert into existing bucket with empty slot
                                let empty_bucket = unsafe { &*empty_bucket_ptr };
                                let entries = unsafe { &mut *empty_bucket.entries.get() };

                                // Use seqlock to protect the write operation (like Go)
                                let seq = empty_bucket.seq.load(Ordering::Relaxed);
                                empty_bucket.seq.store(seq + 1, Ordering::Release); // Start write (make odd)
                                entries[empty_slot].set_hash(hash64);
                                entries[empty_slot].key = MaybeUninit::new(key);
                                entries[empty_slot].val = MaybeUninit::new(new_v.clone());
                                empty_bucket.seq.store(seq + 2, Ordering::Release); // End write (make even)

                                // Update meta to mark slot as occupied
                                let meta = empty_bucket.meta.load(Ordering::Acquire);
                                let new_meta = set_byte(meta, h2, empty_slot);
                                empty_bucket.meta.store(new_meta, Ordering::Release);
                                root.unlock();

                                // Update counter
                                let stripe = hash64 as usize % NUM_STRIPES;
                                table.counters[stripe].fetch_add(1, Ordering::Relaxed);

                                self.maybe_resize_after_insert(table);
                                return (None, Some(new_v));
                            } else {
                                // Need to create new bucket - find the last bucket in chain
                                let mut last_bucket = b;
                                while !last_bucket.next.load(Ordering::Acquire).is_null() {
                                    last_bucket =
                                        unsafe { &*last_bucket.next.load(Ordering::Acquire) };
                                }

                                // Create new bucket
                                let new_bucket =
                                    Box::new(Bucket::single(hash64, h2, key, new_v.clone()));
                                let new_bucket_ptr = Box::into_raw(new_bucket);

                                // Link new bucket
                                last_bucket.next.store(new_bucket_ptr, Ordering::Release);
                                root.unlock();

                                // Update counter
                                let stripe = hash64 as usize % NUM_STRIPES;
                                table.counters[stripe].fetch_add(1, Ordering::Relaxed);

                                self.maybe_resize_after_insert(table);
                                return (None, Some(new_v));
                            }
                        } else {
                            root.unlock();
                            return (None, None);
                        }
                    }
                    Op::Delete => {
                        // Nothing to delete
                        root.unlock();
                        return (None, None);
                    }
                }
            }
        }
    }

    /// RangeProcess iterates through all entries in the map and applies the function `f` to each.
    /// The function can return Op to modify or delete entries.
    /// This method blocks all other operations on the map during execution.
    pub fn range_process<F>(&self, mut f: F)
    where
        F: FnMut(&K, &V) -> (Op, Option<V>),
        V: Clone,
    {
        loop {
            let table = self.seq_load_table();
            let mut to_delete = Vec::new();
            let mut to_update = Vec::new();
            let mut table_swapped = false;

            for i in 0..table.buckets.len() {
                let root = &table.buckets[i];
                root.lock();

                // Check if table was swapped during lock acquisition
                let current_table = self.seq_load_table();
                if !std::ptr::eq(table, current_table) {
                    root.unlock();
                    table_swapped = true;
                    break; // Retry with new table
                }

                let mut b = root;
                loop {
                    let entries = unsafe { &*b.entries.get() };
                    let meta = b.meta.load(Ordering::Acquire);

                    // Iterate through all slots to find occupied ones
                    for slot in 0..ENTRIES_PER_BUCKET {
                        let slot_meta = (meta >> (slot * 8)) & 0xFF;
                        if slot_meta & (SLOT_MASK as u64) != 0 {
                            let entry = &entries[slot];
                            // Double-check with hash field for safety
                            if entry.is_occupied() {
                                let key = unsafe { entry.key.assume_init_ref() };
                                let val = unsafe { entry.val.assume_init_ref() };
                                let (op, new_val) = f(key, val);

                                match op {
                                    Op::Cancel => {
                                        // Continue to next entry
                                    }
                                    Op::Update => {
                                        if let Some(new_v) = new_val {
                                            to_update.push((key.clone(), new_v));
                                        }
                                    }
                                    Op::Delete => {
                                        to_delete.push(key.clone());
                                    }
                                }
                            }
                        }
                    }

                    let next_ptr = b.next.load(Ordering::Acquire);
                    if !next_ptr.is_null() {
                        b = unsafe { &*next_ptr };
                    } else {
                        break;
                    }
                }
                root.unlock();
            }

            if table_swapped {
                continue; // Retry with new table
            }

            // Apply updates and deletes
            for key in to_delete {
                self.remove(key);
            }
            for (key, val) in to_update {
                self.insert(key, val);
            }
            break; // Successfully completed
        }
    }

    pub fn len(&self) -> usize {
        let table_arc = self.seq_load_table();
        let table = &*table_arc;
        self.sum_counters(table)
    }
    pub fn is_empty(&self) -> bool {
        self.len() == 0
    }

    fn seq_load_table(&self) -> &Table<K, V> {
        loop {
            let table_ptr = self.table.load(Ordering::Acquire);
            if table_ptr.is_null() {
                std::thread::yield_now();
                continue;
            }
            return unsafe { &*table_ptr };
        }
    }

    #[inline]
    fn hash_pair(&self, key: &K) -> (u64, u8) {
        use std::hash::Hasher;
        let mut h = self.hasher.build_hasher();
        key.hash(&mut h);
        let hv = h.finish();
        (hv, self.h2(hv))
    }

    #[inline]
    fn h1_mask(&self, hash64: u64, mask: usize) -> usize {
        ((hash64 >> 7) as usize) & mask
    }
    #[inline]
    fn h2(&self, hash64: u64) -> u8 {
        (hash64 as u8) | SLOT_MASK
    }

    fn sum_counters(&self, table: &Table<K, V>) -> usize {
        let mut s = 0usize;
        for i in 0..NUM_STRIPES {
            s += table.counters[i].load(Ordering::Relaxed) as usize;
        }
        s
    }

    fn maybe_resize_after_insert(&self, table: &Table<K, V>) {
        let cap = (table.mask + 1) * ENTRIES_PER_BUCKET;
        let total = self.sum_counters(table);
        let threshold = (cap as f64 * LOAD_FACTOR) as usize;
        if total > threshold {
            self.grow();
        }
    }
    fn maybe_resize_after_remove(&self, table: &Table<K, V>) {
        if !self.shrink_on {
            return;
        }
        let cap = (table.mask + 1) * ENTRIES_PER_BUCKET;
        let total = self.sum_counters(table);
        if cap > MIN_TABLE_LEN * ENTRIES_PER_BUCKET && total * SHRINK_FRACTION < cap {
            self.shrink();
        }
    }

    fn grow(&self) {
        if self
            .resize_lock
            .compare_exchange(false, true, Ordering::Acquire, Ordering::Relaxed)
            .is_err()
        {
            return;
        }
        // 读取旧表引用获取新容量
        let old_ref = self.seq_load_table();
        let new_len = (old_ref.mask + 1) * 2;
        let mut buckets = Vec::with_capacity(new_len);
        for _ in 0..new_len {
            buckets.push(Bucket::new());
        }
        let new_table = Table {
            buckets,
            mask: new_len - 1,
            counters: std::array::from_fn(|_| AtomicU64::new(0)),
        };
        // 将旧数据复制到新表
        self.copy_all(&old_ref, &new_table);

        // 原子替换table指针
        let new_table_ptr = Box::into_raw(Box::new(new_table));
        let old_table_ptr = self.table.swap(new_table_ptr, Ordering::Release);

        // 将旧table添加到old_tables以便稍后清理
        unsafe {
            let old_table = Box::from_raw(old_table_ptr);
            (&mut *self.old_tables.get()).push(old_table);
        }
        self.resize_lock.store(false, Ordering::Release);
    }
    fn shrink(&self) {
        if self
            .resize_lock
            .compare_exchange(false, true, Ordering::Acquire, Ordering::Relaxed)
            .is_err()
        {
            return;
        }
        let old_ref = self.seq_load_table();
        let old_len = old_ref.mask + 1;
        if old_len <= MIN_TABLE_LEN {
            self.resize_lock.store(false, Ordering::Release);
            return;
        }
        let new_len = old_len / 2;
        let mut buckets = Vec::with_capacity(new_len);
        for _ in 0..new_len {
            buckets.push(Bucket::new());
        }
        let new_table = Table {
            buckets,
            mask: new_len - 1,
            counters: std::array::from_fn(|_| AtomicU64::new(0)),
        };
        self.copy_all(&old_ref, &new_table);

        // 原子替换table指针
        let new_table_ptr = Box::into_raw(Box::new(new_table));
        let old_table_ptr = self.table.swap(new_table_ptr, Ordering::Release);

        // 将旧table添加到old_tables以便稍后清理
        unsafe {
            let old_table = Box::from_raw(old_table_ptr);
            (&mut *self.old_tables.get()).push(old_table);
        }
        self.resize_lock.store(false, Ordering::Release);
    }

    // Copy all entries from old table to new table safely under root-bucket lock
    fn copy_all(&self, old: &Table<K, V>, new: &Table<K, V>)
    where
        K: Clone + Eq + std::hash::Hash,
        V: Clone,
    {
        for i in 0..=old.mask {
            let root = &old.buckets[i];
            // Lock the root to stabilize chain while copying
            root.lock();
            let mut b = root;
            loop {
                let entries_ref = unsafe { &*b.entries.get() };
                let meta = b.meta.load(Ordering::Relaxed);
                let mut marked = meta & META_MASK;
                while marked != 0 {
                    let j = first_marked_byte_index(marked);
                    let e = &entries_ref[j];
                    // Check if slot is marked as occupied in meta and entry is occupied
                    if (meta >> (j * 8)) & 0x80 != 0 && e.is_occupied() {
                        let key = unsafe { e.key.assume_init_ref().clone() };
                        let val = unsafe { e.val.assume_init_ref().clone() };
                        let (hash64, h2) = self.hash_pair(&key);
                        self.reinsert_into(new, hash64, h2, key, val);
                    }
                    marked &= marked - 1;
                }
                let next_ptr = b.next.load(Ordering::Acquire);
                if !next_ptr.is_null() {
                    b = unsafe { &*next_ptr };
                } else {
                    break;
                }
            }
            root.unlock();
        }
    }

    // Reinsertion helper used by resize: inserts into provided table
    fn reinsert_into(&self, table: &Table<K, V>, hash64: u64, h2: u8, key: K, val: V)
    where
        K: Clone + Eq,
        V: Clone,
    {
        let idx = self.h1_mask(hash64, table.mask);
        let root = &table.buckets[idx];
        root.lock();
        let mut found_bucket: *const Bucket<K, V> = std::ptr::null();
        let mut found_idx: usize = 0;
        let mut empty_slot: Option<(*const Bucket<K, V>, usize)> = None;
        let mut cur: &Bucket<K, V> = root;
        loop {
            let meta = cur.meta.load(Ordering::Relaxed);
            let h2w = broadcast(h2);
            let mut marked = mark_zero_bytes(meta ^ h2w);
            let entries_ref = unsafe { &*cur.entries.get() };
            while marked != 0 {
                let j = first_marked_byte_index(marked);
                let e = &entries_ref[j];
                // Check hash matching (slot is already marked as occupied)
                if e.is_occupied() && e.equal_hash(hash64) {
                    unsafe {
                        if *e.key.as_ptr() == key {
                            found_bucket = cur as *const _;
                            found_idx = j;
                            break;
                        }
                    }
                }
                marked &= marked - 1;
            }
            if found_bucket.is_null() {
                if empty_slot.is_none() {
                    let empty = (!meta) & META_MASK;
                    if empty != 0 {
                        empty_slot = Some((cur as *const _, first_marked_byte_index(empty)));
                    }
                }
                let next_ptr = cur.next.load(Ordering::Acquire);
                if !next_ptr.is_null() {
                    cur = unsafe { &*next_ptr };
                } else {
                    break;
                }
            } else {
                break;
            }
        }

        if !found_bucket.is_null() {
            unsafe {
                let s = (*found_bucket).seq.load(Ordering::Relaxed);
                (*found_bucket).seq.store(s + 1, Ordering::Release);
                let entries_mut = &mut *(*found_bucket).entries.get();
                // Properly drop the old value before writing new one
                std::ptr::drop_in_place(entries_mut[found_idx].val.as_mut_ptr());
                entries_mut[found_idx].val.as_mut_ptr().write(val.clone());
                (*found_bucket).seq.store(s + 2, Ordering::Release);
            }
            root.unlock();
            return;
        }

        if let Some((eb, ei)) = empty_slot {
            unsafe {
                let entries_mut = &mut *(*eb).entries.get();
                // Always set hash field for occupancy checking
                entries_mut[ei].set_hash(hash64);
                entries_mut[ei].key.as_mut_ptr().write(key);
                entries_mut[ei].val.as_mut_ptr().write(val);
                let new_meta = set_byte((*eb).meta.load(Ordering::Relaxed), h2, ei);
                let s = (*eb).seq.load(Ordering::Relaxed);
                (*eb).seq.store(s + 1, Ordering::Release);
                (*eb).meta.store(new_meta, Ordering::Release);
                (*eb).seq.store(s + 2, Ordering::Release);
            }
            root.unlock();
            let stripe = stripe_index(hash64);
            table.counters[stripe].fetch_add(1, Ordering::Relaxed);
            return;
        }

        // Prepend overflow bucket when no empty slot available
        unsafe {
            let new_bucket = Box::new(Bucket::single(hash64, h2, key, val));
            let new_ptr = Box::into_raw(new_bucket);
            let old_ptr = root.next.load(Ordering::Acquire);
            (*new_ptr).next.store(old_ptr, Ordering::Relaxed);
            root.next.store(new_ptr, Ordering::Release);
        }
        root.unlock();

        let stripe = stripe_index(hash64);
        table.counters[stripe].fetch_add(1, Ordering::Relaxed);
        return;
    }
}

impl<K, V> Bucket<K, V> {
    fn new() -> Self {
        Bucket {
            seq: AtomicU64::new(0),
            meta: AtomicU64::new(0),
            next: AtomicPtr::new(std::ptr::null_mut()),
            entries: UnsafeCell::new(std::array::from_fn(|_| Entry::default())),
        }
    }
    fn single(_hash: u64, h2: u8, key: K, val: V) -> Self {
        let b = Self::new();
        unsafe {
            let entries = &mut *b.entries.get();
            entries[0].set_hash(_hash); // Always set hash for occupancy checking
            entries[0].key.as_mut_ptr().write(key);
            entries[0].val.as_mut_ptr().write(val);
        }
        b.meta.store(set_byte(0, h2, 0), Ordering::Relaxed);
        b
    }

    #[inline]
    fn lock(&self) {
        // Root bucket lock using meta's highest byte bit, independent from seq seqlock
        let mut cur = self.meta.load(Ordering::Acquire);
        loop {
            if (cur & OP_LOCK_MASK) == 0 {
                match self.meta.compare_exchange_weak(
                    cur,
                    cur | OP_LOCK_MASK,
                    Ordering::Acquire,
                    Ordering::Relaxed,
                ) {
                    Ok(_) => break,
                    Err(next) => {
                        cur = next;
                        continue;
                    }
                }
            } else {
                std::hint::spin_loop();
                cur = self.meta.load(Ordering::Acquire);
            }
        }
    }

    #[inline]
    fn unlock(&self) {
        // Release root bucket lock by clearing the bit
        self.meta.fetch_and(!OP_LOCK_MASK, Ordering::Release);
    }
}

fn broadcast(b: u8) -> u64 {
    0x0101_0101_0101_0101u64 * (b as u64)
}
fn first_marked_byte_index(w: u64) -> usize {
    (w.trailing_zeros() >> 3) as usize
}
fn mark_zero_bytes(w: u64) -> u64 {
    (w.wrapping_sub(0x0101_0101_0101_0101)) & (!w) & META_MASK
}
fn set_byte(w: u64, b: u8, idx: usize) -> u64 {
    let shift = (idx as u64) << 3;
    (w & !(0xffu64 << shift)) | ((b as u64) << shift)
}

// Collect clones via for_each to maintain safe iteration semantics under seqlock
fn stripe_index(hash64: u64) -> usize {
    (hash64 as usize) & (NUM_STRIPES - 1)
}
pub fn iter<K: Clone + Eq + std::hash::Hash, V: Clone>(
    map: &FlatMap<K, V>,
) -> impl Iterator<Item = (K, V)> {
    let mut items: Vec<(K, V)> = Vec::new();
    map.for_each(|k, v| {
        items.push((k.clone(), v.clone()));
        true
    });
    items.into_iter()
}

fn try_spin(spins: &mut i32) -> bool {
    // Adaptive backoff: spin briefly, then yield to scheduler, then stop
    if *spins < 50 {
        *spins += 1;
        std::hint::spin_loop();
        true
    } else if *spins < 100 {
        *spins += 1;
        std::thread::yield_now();
        true
    } else {
        false
    }
}

impl<K, V> Drop for FlatMap<K, V> {
    fn drop(&mut self) {
        // Clean up current table
        let table_ptr = self.table.load(Ordering::Acquire);
        if !table_ptr.is_null() {
            unsafe {
                let _ = Box::from_raw(table_ptr);
            }
        }

        // Clean up old tables
        unsafe {
            let old_tables = &mut *self.old_tables.get();
            old_tables.clear();
        }
    }
}

impl<K, V> Drop for Table<K, V> {
    fn drop(&mut self) {
        for b in &self.buckets {
            let mut ptr = b.next.load(Ordering::Acquire);
            while !ptr.is_null() {
                unsafe {
                    let next_ptr = (*ptr).next.load(Ordering::Acquire);
                    let _boxed = Box::from_raw(ptr);
                    ptr = next_ptr;
                }
            }
        }
    }
}

impl<K, V> Entry<K, V> {
    /// Check if this entry is initialized (occupied)
    #[inline]
    fn is_occupied(&self) -> bool {
        // (self.hash & HASH_INIT_FLAG) != 0
        self.hash != 0
    }

    /// Get the actual hash value (without the init flag)
    #[inline]
    fn equal_hash(&self, hash64: u64) -> bool {
        //(self.hash & !HASH_INIT_FLAG) == (hash64 & !HASH_INIT_FLAG)
        self.hash == hash64
    }

    /// Set the hash with the init flag
    #[inline]
    fn set_hash(&mut self, hash64: u64) {
        // Always set the init flag, clear it from input hash first to avoid double-setting
        // self.hash = hash64 | HASH_INIT_FLAG;
        self.hash = hash64.max(1)
        // if hash64 == 0 {
        //     self.hash = 1;
        // } else {
        //     self.hash = hash64;
        // }
    }

    /// Clear the entry (mark as unoccupied)
    #[inline]
    fn clear(&mut self) {
        self.hash = 0; // Clear all bits including init flag
    }
}
